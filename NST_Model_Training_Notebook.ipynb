{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c252e49",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Neural Style Transfer - Model Training Notebook\n",
    "\n",
    "This notebook demonstrates how to **train a Transformer network** for fast neural style transfer.\n",
    "Unlike the original optimization-based method (Gatys et al.), this approach trains a **feedforward model** for each style.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the training pipeline\n",
    "- Learn about content/style/TV losses\n",
    "- Monitor training progress with logs\n",
    "- Save your own style transfer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc7b23",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b326d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from models.definitions.transformer_net import TransformerNet\n",
    "from models.definitions.perceptual_loss_net import PerceptualLossNet\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603d054",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcf6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_dir = os.getcwd()\n",
    "training_config = {\n",
    "    'style_img_name': 'psychedelic.jpg',  # must exist in data/styles/\n",
    "    'content_weight': 1e0,\n",
    "    'style_weight': 2e5,\n",
    "    'tv_weight': 0,\n",
    "    'num_of_epochs': 2,\n",
    "    'subset_size': None,  # Limit data for quick test (10K) / Use None to train on entire dataset\n",
    "    'enable_tensorboard': True,\n",
    "    'image_log_freq': 100,\n",
    "    'console_log_freq': 50,\n",
    "    'checkpoint_freq': 200,\n",
    "    'image_size': 256,\n",
    "    'batch_size': 4,\n",
    "    'dataset_path': os.path.join(base_dir, 'data', 'dataset'),\n",
    "    'style_images_path': os.path.join(base_dir, 'data', 'styles'),\n",
    "    'model_binaries_path': os.path.join(base_dir, 'models', 'binaries'),\n",
    "    'checkpoints_path': os.path.join(base_dir, 'models', 'checkpoints', 'cubism')\n",
    "}\n",
    "\n",
    "os.makedirs(training_config['model_binaries_path'], exist_ok=True)\n",
    "os.makedirs(training_config['checkpoints_path'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a48bb",
   "metadata": {},
   "source": [
    "## üß© Interactive Config: Select Style and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Style Image Selector\n",
    "dropdown_style = widgets.Dropdown(\n",
    "    options=sorted(os.listdir(training_config['style_images_path'])),\n",
    "    value=training_config['style_img_name'],\n",
    "    description='Style Image:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Epochs input\n",
    "slider_epochs = widgets.IntSlider(\n",
    "    value=training_config['num_of_epochs'],\n",
    "    min=1, max=10, step=1,\n",
    "    description='Epochs:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Style weight input\n",
    "style_weight_text = widgets.FloatText(\n",
    "    value=training_config['style_weight'],\n",
    "    description='Style Weight:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Subset size input (None means full dataset)\n",
    "subset_text = widgets.Text(\n",
    "    value=str(training_config['subset_size']) if training_config['subset_size'] is not None else '',\n",
    "    description='Subset Size (leave blank for all):',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Display all widgets together\n",
    "display(dropdown_style, slider_epochs, style_weight_text, subset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f9d98",
   "metadata": {},
   "source": [
    "## üßæ Final Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b027948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config values from user inputs\n",
    "training_config['style_img_name'] = dropdown_style.value\n",
    "training_config['num_of_epochs'] = slider_epochs.value\n",
    "training_config['style_weight'] = style_weight_text.value\n",
    "\n",
    "# Subset size logic\n",
    "if subset_text.value.strip() == '':\n",
    "    training_config['subset_size'] = None\n",
    "else:\n",
    "    try:\n",
    "        training_config['subset_size'] = int(subset_text.value.strip())\n",
    "    except ValueError:\n",
    "        print(\"Invalid subset size. Using full dataset.\")\n",
    "        training_config['subset_size'] = None\n",
    "\n",
    "# Display final training config before starting\n",
    "print(\"üßæ Final Training Configuration:\")\n",
    "for k, v in training_config.items():\n",
    "    print(f\"{k:20s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4875342",
   "metadata": {},
   "source": [
    "## üß† Initialize Model & Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Networks\n",
    "transformer_net = TransformerNet().train().to(device)\n",
    "perceptual_loss_net = PerceptualLossNet(requires_grad=False).to(device)\n",
    "optimizer = Adam(transformer_net.parameters())\n",
    "\n",
    "# Load style image\n",
    "style_path = os.path.join(training_config['style_images_path'], training_config['style_img_name'])\n",
    "style_img = utils.prepare_img(\n",
    "    style_path, target_shape=512, device=device, batch_size=training_config['batch_size']\n",
    ")\n",
    "\n",
    "# Get style features (Gram matrices)\n",
    "style_features = perceptual_loss_net(style_img)\n",
    "target_grams = [utils.gram_matrix(x) for x in style_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c4331",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b584e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.get_training_data_loader(training_config)\n",
    "print(\"Data loaded:\", len(train_loader), \"batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22e268",
   "metadata": {},
   "source": [
    "## üîÅ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "utils.print_header(training_config)\n",
    "\n",
    "acc_content, acc_style, acc_tv = 0, 0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(training_config['num_of_epochs']):\n",
    "    for batch_id, (content_batch, _) in enumerate(train_loader):\n",
    "        content_batch = content_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        stylized_batch = transformer_net(content_batch)\n",
    "        content_feats = perceptual_loss_net(content_batch)\n",
    "        stylized_feats = perceptual_loss_net(stylized_batch)\n",
    "\n",
    "        # Content loss (relu2_2)\n",
    "        content_loss = training_config['content_weight'] * torch.nn.functional.mse_loss(\n",
    "            stylized_feats.relu2_2, content_feats.relu2_2\n",
    "        )\n",
    "\n",
    "        # Style loss\n",
    "        current_grams = [utils.gram_matrix(x) for x in stylized_feats]\n",
    "        style_loss = 0\n",
    "        for g1, g2 in zip(target_grams, current_grams):\n",
    "            style_loss += torch.nn.functional.mse_loss(g1, g2)\n",
    "        style_loss = style_loss * training_config['style_weight'] / len(target_grams)\n",
    "\n",
    "        # Total variation (TV) loss\n",
    "        tv_loss = training_config['tv_weight'] * utils.total_variation(stylized_batch)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = content_loss + style_loss + tv_loss\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate for logs\n",
    "        acc_content += content_loss.item()\n",
    "        acc_style += style_loss.item()\n",
    "        acc_tv += tv_loss.item()\n",
    "\n",
    "        global_step = epoch * len(train_loader) + batch_id\n",
    "\n",
    "        # TensorBoard logs\n",
    "        if training_config['enable_tensorboard']:\n",
    "            writer.add_scalar(\"Loss/Content\", content_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Style\", style_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/TV\", tv_loss.item(), global_step)\n",
    "\n",
    "            if batch_id % training_config['image_log_freq'] == 0:\n",
    "                grid = make_grid(stylized_batch[:4].detach().cpu().clamp(0, 1))\n",
    "                writer.add_image(\"Stylized\", grid, global_step)\n",
    "\n",
    "        # Console log\n",
    "        if batch_id % training_config['console_log_freq'] == 0:\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            print(f\"Epoch {epoch+1}/{training_config['num_of_epochs']} | Batch {batch_id}/{len(train_loader)} | \"\n",
    "                  f\"Elapsed: {elapsed:.2f} min\\nContent: {acc_content:.4f} | Style: {acc_style:.4f} | TV: {acc_tv:.4f}\")\n",
    "            acc_content, acc_style, acc_tv = 0, 0, 0\n",
    "\n",
    "        # Save checkpoints\n",
    "        if training_config['checkpoint_freq'] and (batch_id+1) % training_config['checkpoint_freq'] == 0:\n",
    "            ckpt = utils.get_training_metadata(training_config)\n",
    "            ckpt['state_dict'] = transformer_net.state_dict()\n",
    "            ckpt['optimizer_state'] = optimizer.state_dict()\n",
    "            fname = f\"ckpt_{epoch+1}_{batch_id+1}.pth\"\n",
    "            torch.save(ckpt, os.path.join(training_config['checkpoints_path'], fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e469fa4",
   "metadata": {},
   "source": [
    "## üíæ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = utils.get_training_metadata(training_config)\n",
    "final_model['state_dict'] = transformer_net.state_dict()\n",
    "final_model['optimizer_state'] = optimizer.state_dict()\n",
    "model_name = f\"style_{training_config['style_img_name'].split('.')[0]}_final.pth\"\n",
    "torch.save(final_model, os.path.join(training_config['model_binaries_path'], model_name))\n",
    "print(f\"\\n‚úÖ Final model saved to: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef29941",
   "metadata": {},
   "source": [
    "## üß™ What's Next?\n",
    "\n",
    "- Use `stylization_script.py` or the demo notebook `Image_NST_Notebook` to apply your trained model\n",
    "- Try with different style images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
