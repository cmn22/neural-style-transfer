{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c252e49",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Neural Style Transfer - Model Training Notebook\n",
    "\n",
    "This notebook demonstrates how to **train a Transformer network** for fast neural style transfer.\n",
    "Unlike the original optimization-based method (Gatys et al.), this approach trains a **feedforward model** for each style.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the training pipeline\n",
    "- Learn about content/style/TV losses\n",
    "- Monitor training progress with logs\n",
    "- Save your own style transfer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc7b23",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b326d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from models.definitions.transformer_net import TransformerNet\n",
    "from models.definitions.perceptual_loss_net import PerceptualLossNet\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603d054",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cedcf6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_dir = os.getcwd()\n",
    "training_config = {\n",
    "    'style_img_name': 'psychedelic.jpg',  # must exist in data/styles/\n",
    "    'content_weight': 1e0,\n",
    "    'style_weight': 2e5,\n",
    "    'tv_weight': 0,\n",
    "    'num_of_epochs': 2,\n",
    "    'subset_size': None,  # Limit data for quick test (10K) / Use None to train on entire dataset\n",
    "    'enable_tensorboard': True,\n",
    "    'image_log_freq': 100,\n",
    "    'console_log_freq': 50,\n",
    "    'checkpoint_freq': 200,\n",
    "    'image_size': 256,\n",
    "    'batch_size': 4,\n",
    "    'dataset_path': os.path.join(base_dir, 'data', 'dataset'),\n",
    "    'style_images_path': os.path.join(base_dir, 'data', 'styles'),\n",
    "    'model_binaries_path': os.path.join(base_dir, 'models', 'binaries'),\n",
    "    'checkpoints_path': os.path.join(base_dir, 'models', 'checkpoints', 'cubism')\n",
    "}\n",
    "\n",
    "os.makedirs(training_config['model_binaries_path'], exist_ok=True)\n",
    "os.makedirs(training_config['checkpoints_path'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a48bb",
   "metadata": {},
   "source": [
    "## üß© Interactive Config: Select Style and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e36eddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fc22f9e0f2496fac85984b9d6c5416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Style Image:', index=6, layout=Layout(width='50%'), options=('candy.jpg', 'cubism.jpg', ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d8e1da105b4be48d6ff9cb4c2937e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2, description='Epochs:', layout=Layout(width='50%'), max=10, min=1, style=SliderStyle(descrip‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d87b9ba24cd453cbc42256124324a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=200000.0, description='Style Weight:', layout=Layout(width='50%'), style=DescriptionStyle(desc‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5403d4870f0e481694dfea93484d77af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Subset Size (leave blank for all):', layout=Layout(width='50%'), style=TextStyle(d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Style Image Selector\n",
    "dropdown_style = widgets.Dropdown(\n",
    "    options=sorted(os.listdir(training_config['style_images_path'])),\n",
    "    value=training_config['style_img_name'],\n",
    "    description='Style Image:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Epochs input\n",
    "slider_epochs = widgets.IntSlider(\n",
    "    value=training_config['num_of_epochs'],\n",
    "    min=1, max=10, step=1,\n",
    "    description='Epochs:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Style weight input\n",
    "style_weight_text = widgets.FloatText(\n",
    "    value=training_config['style_weight'],\n",
    "    description='Style Weight:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Subset size input (None means full dataset)\n",
    "subset_text = widgets.Text(\n",
    "    value=str(training_config['subset_size']) if training_config['subset_size'] is not None else '',\n",
    "    description='Subset Size (leave blank for all):',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Display all widgets together\n",
    "display(dropdown_style, slider_epochs, style_weight_text, subset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f9d98",
   "metadata": {},
   "source": [
    "## üßæ Final Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b027948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Final Training Configuration:\n",
      "style_img_name      : psychedelic.jpg\n",
      "content_weight      : 1.0\n",
      "style_weight        : 300000.0\n",
      "tv_weight           : 0\n",
      "num_of_epochs       : 2\n",
      "subset_size         : 100\n",
      "enable_tensorboard  : True\n",
      "image_log_freq      : 100\n",
      "console_log_freq    : 50\n",
      "checkpoint_freq     : 200\n",
      "image_size          : 256\n",
      "batch_size          : 4\n",
      "dataset_path        : /Users/chaitanyamalani/Desktop/CMN/Study/Projects/neural-style-transfer/data/dataset\n",
      "style_images_path   : /Users/chaitanyamalani/Desktop/CMN/Study/Projects/neural-style-transfer/data/styles\n",
      "model_binaries_path : /Users/chaitanyamalani/Desktop/CMN/Study/Projects/neural-style-transfer/models/binaries\n",
      "checkpoints_path    : /Users/chaitanyamalani/Desktop/CMN/Study/Projects/neural-style-transfer/models/checkpoints/cubism\n"
     ]
    }
   ],
   "source": [
    "# Update config values from user inputs\n",
    "training_config['style_img_name'] = dropdown_style.value\n",
    "training_config['num_of_epochs'] = slider_epochs.value\n",
    "training_config['style_weight'] = style_weight_text.value\n",
    "\n",
    "# Subset size logic\n",
    "if subset_text.value.strip() == '':\n",
    "    training_config['subset_size'] = None\n",
    "else:\n",
    "    try:\n",
    "        training_config['subset_size'] = int(subset_text.value.strip())\n",
    "    except ValueError:\n",
    "        print(\"Invalid subset size. Using full dataset.\")\n",
    "        training_config['subset_size'] = None\n",
    "\n",
    "# Display final training config before starting\n",
    "print(\"üßæ Final Training Configuration:\")\n",
    "for k, v in training_config.items():\n",
    "    print(f\"{k:20s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4875342",
   "metadata": {},
   "source": [
    "## üß† Initialize Model & Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ebcc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyamalani/Desktop/CMN/Study/Projects/neural-style-transfer/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/chaitanyamalani/Desktop/CMN/Study/Projects/neural-style-transfer/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Networks\n",
    "transformer_net = TransformerNet().train().to(device)\n",
    "perceptual_loss_net = PerceptualLossNet(requires_grad=False).to(device)\n",
    "optimizer = Adam(transformer_net.parameters())\n",
    "\n",
    "# Load style image\n",
    "style_path = os.path.join(training_config['style_images_path'], training_config['style_img_name'])\n",
    "style_img = utils.prepare_img(\n",
    "    style_path, target_shape=512, device=device, batch_size=training_config['batch_size']\n",
    ")\n",
    "\n",
    "# Get style features (Gram matrices)\n",
    "style_features = perceptual_loss_net(style_img)\n",
    "target_grams = [utils.gram_matrix(x) for x in style_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c4331",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b584e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 200 datapoints (50 batches) for training.\n",
      "Data loaded: 25 batches\n"
     ]
    }
   ],
   "source": [
    "train_loader = utils.get_training_data_loader(training_config)\n",
    "print(\"Data loaded:\", len(train_loader), \"batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22e268",
   "metadata": {},
   "source": [
    "## üîÅ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f45c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning the style of psychedelic.jpg style image.\n",
      "********************************************************************************\n",
      "Hyperparams: content_weight=1.0, style_weight=300000.0, tv_weight=0\n",
      "********************************************************************************\n",
      "Logging to console every 50 batches.\n",
      "Saving checkpoint models every 200 batches.\n",
      "Tensorboard enabled.\n",
      "Run \"tensorboard --logdir=runs --samples_per_plugin images=50\"\n",
      "Open http://localhost:6006/ in your browser.\n",
      "********************************************************************************\n",
      "Epoch 1/2 | Batch 0/25 | Elapsed: 0.01 min\n",
      "Content: 11.8847 | Style: 102.1060 | TV: 0.0000\n",
      "Epoch 2/2 | Batch 0/25 | Elapsed: 0.11 min\n",
      "Content: 191.8587 | Style: 17789.1855 | TV: 0.0000\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "utils.print_header(training_config)\n",
    "\n",
    "acc_content, acc_style, acc_tv = 0, 0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(training_config['num_of_epochs']):\n",
    "    for batch_id, (content_batch, _) in enumerate(train_loader):\n",
    "        content_batch = content_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        stylized_batch = transformer_net(content_batch)\n",
    "        content_feats = perceptual_loss_net(content_batch)\n",
    "        stylized_feats = perceptual_loss_net(stylized_batch)\n",
    "\n",
    "        # Content loss (relu2_2)\n",
    "        content_loss = training_config['content_weight'] * torch.nn.functional.mse_loss(\n",
    "            stylized_feats.relu2_2, content_feats.relu2_2\n",
    "        )\n",
    "\n",
    "        # Style loss\n",
    "        current_grams = [utils.gram_matrix(x) for x in stylized_feats]\n",
    "        style_loss = 0\n",
    "        for g1, g2 in zip(target_grams, current_grams):\n",
    "            style_loss += torch.nn.functional.mse_loss(g1, g2)\n",
    "        style_loss = style_loss * training_config['style_weight'] / len(target_grams)\n",
    "\n",
    "        # Total variation (TV) loss\n",
    "        tv_loss = training_config['tv_weight'] * utils.total_variation(stylized_batch)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = content_loss + style_loss + tv_loss\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate for logs\n",
    "        acc_content += content_loss.item()\n",
    "        acc_style += style_loss.item()\n",
    "        acc_tv += tv_loss.item()\n",
    "\n",
    "        global_step = epoch * len(train_loader) + batch_id\n",
    "\n",
    "        # TensorBoard logs\n",
    "        if training_config['enable_tensorboard']:\n",
    "            writer.add_scalar(\"Loss/Content\", content_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Style\", style_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/TV\", tv_loss.item(), global_step)\n",
    "\n",
    "            if batch_id % training_config['image_log_freq'] == 0:\n",
    "                grid = make_grid(stylized_batch[:4].detach().cpu().clamp(0, 1))\n",
    "                writer.add_image(\"Stylized\", grid, global_step)\n",
    "\n",
    "        # Console log\n",
    "        if batch_id % training_config['console_log_freq'] == 0:\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            print(f\"Epoch {epoch+1}/{training_config['num_of_epochs']} | Batch {batch_id}/{len(train_loader)} | \"\n",
    "                  f\"Elapsed: {elapsed:.2f} min\\nContent: {acc_content:.4f} | Style: {acc_style:.4f} | TV: {acc_tv:.4f}\")\n",
    "            acc_content, acc_style, acc_tv = 0, 0, 0\n",
    "\n",
    "        # Save checkpoints\n",
    "        if training_config['checkpoint_freq'] and (batch_id+1) % training_config['checkpoint_freq'] == 0:\n",
    "            ckpt = utils.get_training_metadata(training_config)\n",
    "            ckpt['state_dict'] = transformer_net.state_dict()\n",
    "            ckpt['optimizer_state'] = optimizer.state_dict()\n",
    "            fname = f\"ckpt_{epoch+1}_{batch_id+1}.pth\"\n",
    "            torch.save(ckpt, os.path.join(training_config['checkpoints_path'], fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e469fa4",
   "metadata": {},
   "source": [
    "## üíæ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b89bb565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final model saved to: style_psychedelic_final.pth\n"
     ]
    }
   ],
   "source": [
    "final_model = utils.get_training_metadata(training_config)\n",
    "final_model['state_dict'] = transformer_net.state_dict()\n",
    "final_model['optimizer_state'] = optimizer.state_dict()\n",
    "model_name = f\"style_{training_config['style_img_name'].split('.')[0]}_final.pth\"\n",
    "torch.save(final_model, os.path.join(training_config['model_binaries_path'], model_name))\n",
    "print(f\"\\n‚úÖ Final model saved to: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef29941",
   "metadata": {},
   "source": [
    "## üß™ What's Next?\n",
    "\n",
    "- Use `stylization_script.py` or the demo notebook `Image_NST_Notebook` to apply your trained model\n",
    "- Try with different style images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
