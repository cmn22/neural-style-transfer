{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb96dbf",
   "metadata": {},
   "source": [
    "# üé® Video Neural Style Transfer - Interactive Notebook\n",
    "\n",
    "This notebook allows you to apply a feedforward neural style transfer model to a video.\n",
    "\n",
    "You can:\n",
    "- Choose a video file from the input directory or upload your own\n",
    "- Select a pretrained style model\n",
    "- Adjust the width and smoothing settings\n",
    "- Stylize the video and download the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea633319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Video\n",
    "from utils.utils import frame_to_tensor, post_process_image, print_model_metadata\n",
    "from utils.jupyter_parsing import parse_uploaded_file\n",
    "from models.definitions.transformer_net import TransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1d912",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Paths and Video Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4623c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "content_path = \"data/input\"\n",
    "output_path = \"data/output\"\n",
    "model_path = \"models/binaries\"\n",
    "\n",
    "os.makedirs(content_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "available_videos = sorted([f for f in os.listdir(content_path) if f.lower().endswith(('.mp4', '.mov'))])\n",
    "available_styles = sorted([f for f in os.listdir(model_path) if f.endswith('.pth')])\n",
    "\n",
    "# Upload or choose video\n",
    "use_uploaded = widgets.Checkbox(value=False, description=\"Upload your own video\")\n",
    "uploader = widgets.FileUpload(accept=\".mp4,.mov\", multiple=False)\n",
    "\n",
    "video_dropdown = widgets.Dropdown(options=available_videos, description=\"Choose Video:\")\n",
    "style_dropdown = widgets.Dropdown(options=available_styles, description=\"Choose Style:\")\n",
    "\n",
    "# Parameter controls\n",
    "smoothing_slider = widgets.FloatSlider(value=0.3, min=0.0, max=1.0, step=0.05, description=\"Smoothing:\")\n",
    "verbose_checkbox = widgets.Checkbox(value=False, description=\"Verbose\")\n",
    "\n",
    "# Display widgets\n",
    "display(use_uploaded, uploader)\n",
    "display(video_dropdown, style_dropdown, smoothing_slider, verbose_checkbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dcf1d",
   "metadata": {},
   "source": [
    "## üìÇ Resolve Video and Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_uploaded.value and uploader.value:\n",
    "    input_video_name = parse_uploaded_file(uploader, content_path)\n",
    "    print(f\"Uploaded video saved as: {input_video_name}\")\n",
    "else:\n",
    "    input_video_name = video_dropdown.value\n",
    "\n",
    "style_model_name = style_dropdown.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220dfcaa",
   "metadata": {},
   "source": [
    "## üßê Stylize the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da320be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = TransformerNet().to(device)\n",
    "checkpoint = torch.load(os.path.join(model_path, style_model_name), map_location=device)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "if verbose_checkbox.value:\n",
    "    print_model_metadata(checkpoint)\n",
    "\n",
    "# Open video\n",
    "video_path = os.path.join(content_path, input_video_name)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Prepare output writer\n",
    "output_filename = f\"styled_{os.path.splitext(input_video_name)[0]}_{os.path.splitext(style_model_name)[0]}.mp4\"\n",
    "output_path_full = os.path.join(output_path, output_filename)\n",
    "out_writer = cv2.VideoWriter(output_path_full, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "# Stylize video frame by frame\n",
    "prev_stylized = None\n",
    "alpha = smoothing_slider.value\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(total_frames), desc=\"Stylizing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        tensor = frame_to_tensor(rgb, device, should_normalize=True)\n",
    "        output_tensor = model(tensor).cpu().numpy()[0]\n",
    "        stylized = post_process_image(output_tensor)\n",
    "\n",
    "        if prev_stylized is not None and alpha > 0:\n",
    "            stylized = cv2.addWeighted(stylized, 1 - alpha, prev_stylized, alpha, 0)\n",
    "        prev_stylized = stylized.copy()\n",
    "\n",
    "        bgr_output = cv2.cvtColor(stylized, cv2.COLOR_RGB2BGR)\n",
    "        out_writer.write(bgr_output)\n",
    "\n",
    "cap.release()\n",
    "out_writer.release()\n",
    "\n",
    "print(f\"\\nStylized video saved to: {output_path_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01295e8",
   "metadata": {},
   "source": [
    "## üé• View Output Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(output_path_full, embed=True, width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef53b3",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Note on Video Playback\n",
    "\n",
    "Due to browser and Jupyter notebook limitations, certain video formats or encodings (especially ones not using H.264) may not play correctly **inside the notebook**, especially when using VSCode or some Jupyter environments.\n",
    "\n",
    "However, the **stylized video is correctly saved to disk** and should play normally when opened with:\n",
    "- File Explorer / Finder\n",
    "- VLC or any standard media player\n",
    "- Browsers (after converting with H.264 using `ffmpeg`)\n",
    "\n",
    "If playback fails inside the notebook, try running this in terminal to re-encode:\n",
    "```bash\n",
    "ffmpeg -i your_video.mp4 -vcodec libx264 -pix_fmt yuv420p compatible_video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bfcae9",
   "metadata": {},
   "source": [
    "## üî≠ Future Improvements\n",
    "\n",
    "### 1. ‚ú® Model: From FastFeed to Temporal-Aware Networks\n",
    "\n",
    "The current approach uses a **Feedforward Transformer model** trained on static images. While it‚Äôs fast, it lacks **temporal awareness** ‚Äî meaning:\n",
    "\n",
    "- Each frame is stylized independently\n",
    "- May cause **flickering or inconsistency** between frames\n",
    "- Not ideal for smooth video playback\n",
    "\n",
    "#### ‚úÖ Better Alternative: Temporal Loss (Video Style Transfer)\n",
    "\n",
    "Temporal-aware models (e.g., **ReCoNet**, **STROTSS**, or models with optical flow tracking) can:\n",
    "\n",
    "- Compare stylized current frame with previous frame\n",
    "- Minimize differences using **temporal consistency loss**\n",
    "- Result in **visually smoother and consistent output**, like a real-time artistic video\n",
    "\n",
    "These models are **trained differently** (with consecutive frames), so existing `.pth` files from static NST **won‚Äôt work**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üß† Advanced Features: Semantic Segmentation\n",
    "\n",
    "Adding **segmentation** allows **selective stylization**, e.g.:\n",
    "\n",
    "- Style the background differently from foreground\n",
    "- Preserve faces or moving objects\n",
    "- Combine different styles per object/region\n",
    "\n",
    "This can be done by:\n",
    "- Using **pretrained semantic segmentation models** (e.g., DeepLabv3)\n",
    "- Creating masks per frame\n",
    "- Blending stylized and original content accordingly\n",
    "\n",
    "This adds:\n",
    "- More control and creativity\n",
    "- But also more compute & complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
